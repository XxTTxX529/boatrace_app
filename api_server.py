import ssl
import requests
from bs4 import BeautifulSoup
from flask import Flask, jsonify, request
from flask_cors import CORS
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

# **SSLè¨¼æ˜æ›¸ã®æ¤œè¨¼ã‚’ç„¡åŠ¹åŒ–**
ssl._create_default_https_context = ssl._create_unverified_context

app = Flask(__name__)
CORS(app)  # CORSè¨­å®šè¿½åŠ 

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36",
    "Referer": "https://www.boatrace.jp/",
    "Accept-Language": "ja,en;q=0.9",
}

BOAT_RACE_CODES = {
    "01": "æ¡ç”Ÿ", "02": "æˆ¸ç”°", "03": "æ±Ÿæˆ¸å·", "04": "å¹³å’Œå³¶", "05": "å¤šæ‘©å·",
    "06": "æµœåæ¹–", "07": "è’²éƒ¡", "08": "å¸¸æ»‘", "09": "æ´¥", "10": "ä¸‰å›½",
    "11": "ã³ã‚ã“", "12": "ä½ä¹‹æ±Ÿ", "13": "å°¼å´", "14": "é³´é–€", "15": "ä¸¸äº€",
    "16": "å…å³¶", "17": "å®®å³¶", "18": "å¾³å±±", "19": "ä¸‹é–¢", "20": "è‹¥æ¾",
    "21": "èŠ¦å±‹", "22": "ç¦å²¡", "23": "ã‹ã‚‰ã¤", "24": "å¤§æ‘"
}

# **é–‹å‚¬ãƒ¬ãƒ¼ã‚¹ã®URLã‚’å–å¾—**
def get_race_links():
    url = "https://www.boatrace.jp/owpc/pc/race/index"
    print(f"ğŸ”„ é–‹å‚¬æƒ…å ±ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {url}")
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        race_links = {}
        race_table = soup.select("a[href*='racelist']")

        for link in race_table:
            href = link["href"]
            jcd = href.split("jcd=")[-1].split("&")[0]
            if jcd in BOAT_RACE_CODES:
                race_links[BOAT_RACE_CODES[jcd]] = f"https://www.boatrace.jp{href}"

        return race_links
    except requests.exceptions.RequestException as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ¬ãƒ¼ã‚¹æƒ…å ±å–å¾—å¤±æ•—ï¼‰: {e}")
        return {}

# **æ¬ å ´é¸æ‰‹ã®ãƒã‚§ãƒƒã‚¯**
def check_missing_race(url, place_name, race_no):
    print(f"ğŸ”„ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ä¸­: {url}")
    try:
        response = requests.get(url, headers=HEADERS, timeout=5)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        black_elements = soup.select(".is-miss")

        if black_elements:
            print(f"âœ… æ¬ å ´ã‚ã‚Š: {url} ({place_name} ãƒ¬ãƒ¼ã‚¹{race_no})")
            return place_name, race_no, url
    except requests.exceptions.Timeout:
        print(f"â³ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}ï¼ˆå†è©¦è¡Œã—ã¾ã›ã‚“ï¼‰")
    except requests.exceptions.RequestException as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {url} â†’ {e}")
    return place_name, None, None  # æ¬ å ´ãªã—

# **APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆæ¬ å ´æƒ…å ±å–å¾—ï¼‰**
@app.route("/api/missing-races", methods=["GET"])
def get_missing_races():
    date = request.args.get("date", None)  # ?date=YYYYMMDD ã§éå»ãƒ‡ãƒ¼ã‚¿å–å¾—å¯èƒ½
    race_links = get_race_links()
    missing_races = {place: [] for place in BOAT_RACE_CODES.values()}  # åˆæœŸåŒ–

    urls = [
        (f"https://www.boatrace.jp/owpc/pc/race/racelist?rno={rno}&jcd={code}&hd={date or datetime.today().strftime('%Y%m%d')}", place, rno)
        for code, place in BOAT_RACE_CODES.items() for rno in range(1, 13)
    ]

    with ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(lambda x: check_missing_race(x[0], x[1], x[2]), urls)

    for place, race_no, url in results:
        if race_no:
            missing_races[place].append((race_no, url))

    return jsonify({
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "missing_races": missing_races,
        "race_links": race_links,
    })

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5001, threaded=True, debug=True)